{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Prepare the input dataset\n",
    "## 1. Download the dataset\n",
    "The first step of this  project is to obtain the historical data of stock price. Financial data can be expensive and hard to extract, that's why in this eperiment we use the Python library `quandl` to obtain such information. This library has been chosen since it is easy to use and it provides a limited number of free queries per day. Quandl is an API,and the Python library is a wrapper over the APIs. To see a sample data returned by this API, run the folowing command in our prompt:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "curl \"https://www.quandl.com/api/v3/datasets/WIKI/FB/data.csv\"\n",
    "Date,Open,High,Low,Close,Volume,Ex-Dividend,Split Ratio,Adj. Open,Adj. High,Adj. Low,Adj. Close,Adj. Volume\n",
    "2018-03-27,156.31,162.85,150.75,152.19,76787884.0,0.0,1.0,156.31,162.85,150.75,152.19,76787884.0\n",
    "2018-03-26,160.82,161.1,149.02,160.06,125438294.0,0.0,1.0,160.82,161.1,149.02,160.06,125438294.0\n",
    "2018-03-23,165.44,167.1,159.02,159.39,52306891.0,0.0,1.0,165.44,167.1,159.02,159.39,52306891.0\n",
    "2018-03-22,166.13,170.27,163.72,164.89,73389988.0,0.0,1.0,166.13,170.27,163.72,164.89,73389988.0\n",
    "2018-03-21,164.8,173.4,163.3,169.39,105350867.0,0.0,1.0,164.8,173.4,163.3,169.39,105350867.0\n",
    "2018-03-20,167.47,170.2,161.95,168.15,128925534.0,0.0,1.0,167.47,170.2,161.95,168.15,128925534.0\n",
    "2018-03-19,177.01,177.17,170.06,172.56,86897749.0,0.0,1.0,177.01,177.17,170.06,172.56,86897749.0\n",
    "2018-03-16,184.49,185.33,183.41,185.09,23090480.0,0.0,1.0,184.49,185.33,183.41,185.09,23090480.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is a CSV, and each line contains the date, the opening price, the highest and the lowest of the day, the closing , the adusted, and some volumes. The lines are sorted from the most recent to the least. The columns we are intested in is the **Adj. Close**, which is the closing price after adjustment. \n",
    "\n",
    "Let's build a Python function to extract the adjusted price using the Quandl APIs. The function we are looking for should be able to cache calls and specify an initial and final timestamp to get the historical data beyond the symbol. Here's the code to do so, which is put in the `tools.py` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quandl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-30c9fb8e0953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'quandl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import quandl\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "##################################\n",
    "# DOWNLOAD DATASETS\n",
    "##################################\n",
    "\n",
    "def date_obj_to_str(date_obj):\n",
    "    return date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def save_pickle(something, path):\n",
    "    if not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "    with open(path, 'wb') as fh:\n",
    "        pickle.dump(something, fh, pickle.DEFAULT_PROTOCOL)\n",
    "        \n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as fh:\n",
    "        pkl = pickle.load(fh)\n",
    "    return pkl\n",
    "   \n",
    "    \n",
    "def fetch_stock_price(symbol, from_date, to_date, che_path=\"./tmp/prices/\"):\n",
    "    assert(from_date <= to_date)\n",
    "    \n",
    "    filename = \"{}_{}_{}.pk\".format(symbol, str(from_date), str(to_date))\n",
    "    price_filepath = os.path.join(cache_path, filename)\n",
    "\n",
    "    try: \n",
    "        prices = load_pickle(price_filepath)\n",
    "        print(\"loaded from\", price_filepath)\n",
    "\n",
    "    except IOError:\n",
    "        historic = quandl.get(\"WIKI/\" + symbol,\n",
    "                                         start_date=date_obj_to_str(from_date),\n",
    "                                         end_date=date_obj_to_str(to_date))\n",
    "\n",
    "        prices = historic[\"Adj. Close\"].tolist()\n",
    "        save_pickle(prices, price_filepath)\n",
    "        print(\"saved into\", price_filepath)\n",
    "\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object of the function `fetch_stock_price` is a mono-dimensional array, containing the stock price for the requested symbol, ordered from the *from_date* to the *to_date*. Caching is done within the funciton, which means if a cache is missed, then the *quandl* API is called. The `date_obj_to_str` function is just a helper function, to convert *datetime.date* to the correct string format needed for the API.\n",
    "\n",
    "To validate the `fetch_stock_price` function, let's print the adjusted price of the Google stock price (whose symbol is GOOG) for January 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fetch_stock_price(\"GOOG\", datetime.date(2018,1,1), datetime.date(2018,1,31)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output, which is the stock price of Google in January 2018, is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "[786.14, 786.9, 794.02, 806.15, 806.65, 804.79, 807.91, 806.36, 807.88, 804.61, 806.07, 802.175, 805.02, 819.31,823.87, 835.67, 832.15, 823.31, 802.32, 796.79]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format the dataset\n",
    "\n",
    "In order to feed in the machine-learning models, the input data needs to be in the form of multiple observations with a number of feature size. Since timeseries data is mono-dimensional array, we don't have a such pre-defined length. Therfore, instead of varying the number of features, we will change the number of observations, maintaining a constant feature size. Each observation represents a temporal window of the timeseries, and by sliding the window of one position on the right, we create another observation. Here is the code to do so (still in `tools.py` script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# FORMAT THE DATASETS\n",
    "##########################################\n",
    "\n",
    "def format_dataset(values, temporal_features):\n",
    "    feat_splits = [values[i: i+temporal_features] for i in range(len(values) - temporal_features)]\n",
    "    feats = np.vstack(feat_splits)\n",
    "    labels = np.array(values[temporal_features:])\n",
    "    return feats, labels\n",
    "\n",
    "\n",
    "# Function to reshape matrices to mono-dimensional (1D) array\n",
    "def matrix_to_array(m):\n",
    "    return np.asarray(m).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the timeseries, and the feature size, the function creates a sliding window which sweeps the timeseries, producing features and labels (that is, the value following the end of sliding window, at each iteration). Finally, all the observations are pile up vertically, as well as the labels. The outcome is an observation with a defined number of columns, and a label vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' visualize the stock prices of some most popular companies in the United States in two years: 2015 and 2016. Feel free to change the **symbols** and **date** to visualize your favorite company'stocks in different time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "symbols = [\"MSFT\", \"KO\", \"AAL\", \"MMM\", \"AXP\", \"GE\", \"GM\", \"JPM\", \"UPS\"] # companies's code\n",
    "ax = plt.subplot(1,1,1)\n",
    "for sym in symbols:\n",
    "    prices = fetch_stock_price(sym, datetime.date(2015, 1,1), datetime.date(2016, 12,31))\n",
    "    ax.plot(range(len(prices)), prices, label=sym)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "plt.xlabel(\"Trading days since 2015-1-1\")\n",
    "plt.ylabel(\"Stock price [$]\")\n",
    "plt.title(\"Prices of some American stocks in trading days of 2015 and 2016\")\n",
    "plt.savefig(\"./graph/stock_prices_2015_2016.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is shown below:\n",
    "![US_stock_prices_2015_2016](./graph/stock_prices_2015_2016.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Stock Prices Prediction using Regression\n",
    "Given the observation matrix and a real value label, we first approach the problem as a regression problem. However, this approach is not ideal in the case of timeseries data. Treating the problem as a regression problem, we force the algorithm to think that they each feature is independent, while instead, they are correlated, since they are the window of the same timeseries. Anyway, let's start with this simple assumption (each feature is independent),and see later how it can be improved by exploiting the temporal correlation.\n",
    "\n",
    "## 1. Functions to evaluate model\n",
    "In order to evaluate the model, we create a function that, given the observation matrix, the true labels, and the predicted ones, will output the metrics in term of **mean_square_error (MSE)** and **mean absolute error (MAE)** of the prediction. It will also plot the training, testing, and predicted timeseries one onto another, to visually check the performance. The function is put into the `evaluate_ts.py` file, so other scripts can access it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from tools import matrix_to_array\n",
    "\n",
    "def evaluate_ts(features, y_true, y_pred):\n",
    "    print(\"Evaluation of the predictions:\")\n",
    "    print(\"MSE: \", np.mean(np.square(y_true - y_pred)))\n",
    "    print(\"mae: \", np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "    print(\"Benchmark: if prediction == last feature (prediction without any model)\")\n",
    "    print(\"MSE: \", np.mean(np.square(features[:,-1] - y_true)))\n",
    "    print(\"mae: \", np.mean(np.abs(features[:,-1] - y_true)))\n",
    "\n",
    "    plt.plot(matrix_to_array(y_true), 'b')\n",
    "    plt.plot(matrix_to_array(y_pred), 'r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Predicted and true values\")\n",
    "    plt.title(\"Predicted (Red) VS Real (Blue)\")\n",
    "    plt.show()\n",
    "\n",
    "    error = np.abs(matrix_to_array(y_pred) - matrix_to_array(y_true))\n",
    "    plt.plot(error, 'r')\n",
    "    fit = np.polyfit(range(len(error)), error, deg=1)\n",
    "    plt.plot(fit[0] * range(len(error))+ fit[1], '--')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Prediction error L1 norm\")\n",
    "    plt.title(\"Prediction error (absolute) and trendline\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the result, we also include the benchmark metrics when no model is used for prediction in the function above. It means that the day-after stock value is simply predicted as the value of present day (in the stock market, this means that the price of stock for tomorrow is predicted the same as the price that stock has today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to move to the modelling phase. The following code are put inside the `regression_stock_price.py`. Let's start with some imports and with the seed for `numpy` and `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from evaluate_ts import evaluate_ts\n",
    "from tools import fetch_stock_price, format_dataset\n",
    "\n",
    "tf.reset_default_graph() # Clear the default graph and resets the global default graph\n",
    "tf.set_random_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the stock price dataset and transform it into an observation matrix. In this example, we use 20 as feature size, since it's roughly equivalent to the working days in a month. The regression problem has now shaped this way: given the 20 values of the cosine in the past, forecast the next day value.\n",
    "\n",
    "Let's use the Apple's stock price in this example, whose symbol is **AAPL**. We have one year of training data (2015) which will be used to predict the stock price for the whole year of 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the dataset creation\n",
    "symbol = \"AAPL\"\n",
    "feat_dimension = 20\n",
    "train_size = 252\n",
    "test_size = 252 - feat_dimension\n",
    "\n",
    "# Fetch the values, and prepare the  train/test split\n",
    "stock_values = fetch_stock_price(symbol, datetime.date(2016,1,1), datetime.date(2017,12,31))\n",
    "minibatch_X, minibatch_y = format_dataset(stock_values, feat_dimension)\n",
    "\n",
    "train_X = minibatch_X[:train_size, :].astype(np.float32)\n",
    "train_y = minibatch_y[:train_size].reshape((-1,1)).astype(np.float32)\n",
    "test_X = minibatch_X[train_size:, :].astype(np.float32)\n",
    "test_y = minibatch_y[train_size:].reshape((-1,1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset, let's define the placeholders for the observation matrix and the labesl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the place holders\n",
    "X_tf = tf.placeholder(\"float\", shape=(None, feat_dimension), name=\"X\")\n",
    "y_tf = tf.placeholder(\"float\", shape = (None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in this part of the script, we will define some parameters for Tensorflow. More specifically: the learning rate, the type of optimizer to use, and the number of *epoch*. (that is, how many times the training dataset goes into the learner during the training operation). Feel free to change them if you want to achieve better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters for the tensorflow model\n",
    "learning_rate = 0.5\n",
    "optimizer = tf.train.AdamOptimizer\n",
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model is implemented in the most classical way, that is, the multiplication between the observation matrix with a weights array plus the bias. The output of the model is an array containing the predictions for all the observations contained in *x*. This model is wrapped inside a function called `regression_ANN` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the simple linear regression model\n",
    "def regression_ANN(x, weights, biases):\n",
    "    return tf.add(biases, tf.matmul(x, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to defined the placeholders for the trainable parameters,which are the weights and biases. In addtion, we also want to see the predictions, the cost and the training operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([feat_dimension, 1], mean=0.0, stddev=1.0), name=\"weights\")\n",
    "biases = tf.Variable(tf.zeros([1,1]), name=\"bias\")\n",
    "\n",
    "# predictions, cost and operator\n",
    "y_pred = regression_ANN(X_tf, weights, biases)\n",
    "cost = tf.reduce_mean(tf.square(y_tf - y_pred))\n",
    "train_op = optimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now ready to open a `tensorflow` session, and train the model. We first initialize the variables, then, in a loop, we will feed the *training* dataset into the *tensorflow* graph. At each iteration, we will print the training Mean Squared Error (MSE). After the training, we evaluated the MSE on the testing dataset, and finally plotted the performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # For each epoch, the whole training set is feeded into the tensorflow graph\n",
    "    for i in range(n_epochs):\n",
    "        train_cost, _ = sess.run([cost, train_op], feed_dict={X_tf: train_X, y_tf: train_y})\n",
    "        print(\"Training iteration\", i, \"MSE\", train_cost)\n",
    "\n",
    "    # After the training, check performance of the test set\n",
    "    test_cost, y_pr = sess.run([cost, y_pred], feed_dict={X_tf: test_X, y_tf: test_y})\n",
    "    print(\"Test dataset:\", test_cost)\n",
    "\n",
    "    # Evaluate the results\n",
    "    evaluate_ts(test_X, test_y, y_pr)\n",
    "\n",
    "    # Visualize the predicted values\n",
    "    plt.plot(range(len(stock_values)), stock_values, 'b')\n",
    "    plt.plot(range(len(stock_values)-test_size, len(stock_values)), y_pr, 'r--')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Predicted and true values\")\n",
    "    plt.title(\"Predicted (Red) VS Real Blue\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The output after running the script should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "Training iteration 0 MSE 100820.46\n",
    "Training iteration 1 MSE 7751.4536\n",
    "Training iteration 2 MSE 11512.154\n",
    "Training iteration 3 MSE 41743.094\n",
    "Training iteration 4 MSE 42018.547\n",
    "Training iteration 5 MSE 22074.584\n",
    "Training iteration 6 MSE 4263.945\n",
    ". . .\n",
    ". . .\n",
    ". . .\n",
    "Training iteration 9994 MSE 3.7425663\n",
    "Training iteration 9995 MSE 3.742465\n",
    "Training iteration 9996 MSE 3.7423615\n",
    "Training iteration 9997 MSE 3.74226\n",
    "Training iteration 9998 MSE 3.742161\n",
    "Training iteration 9999 MSE 3.7420602\n",
    "Test dataset: 2.028827\n",
    "Evaluation of the predictions:\n",
    "MSE (mean squared error):  2.028827\n",
    "MAE (mean absolute error):  1.0663399\n",
    "Benchmark: if prediction == last feature (prediction without any model)\n",
    "MSE:  124.76326\n",
    "MAE:  9.082401\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training performance and testing performance are quite similar, therefore we're not overfitting the model. The MSE when using linear regressor is much better than that of the case no model at all. At the beginning, the cost is really high, but after many iterations, it gets very close to zero. \n",
    "\n",
    "The MAE here can be interpreted as dollars. With a learned, we would have predicted on average one dollar (**1.0663399**) closer to the real price in the day after. Meanwhile, without any learned, the cost is nine times higher (**9.082401**). \n",
    "\n",
    "Let's visualize the predicted values versus the real values on test set:\n",
    "![predicted_vs_real_regression](./graph/regression/predicted_vs_real_values_test_set.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is absolute error, with the trend line (dotted):\n",
    "![abs_error_trendline](./graph/regression/predictions_error_trendline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the real and predicted value in both the train and test set:\n",
    "![predictions_on_train_test](./graph/regression/prediction_on_train_test_set.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a simple regression algorithm can achieve such an impressive result. However, as mention in the beginning, this approach is not ideal since each feature is treated as independent. In the next section, we will discover how to exploit the correlation between features to peform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Stock Price Prediction using Long short-term memory - LSTM 101\n",
    "## 1. LSTM Model\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** model is a special case of RNNs, Recurrent Neural Networks. Basically, RNN works on sequential data: they accept multidimensional signals as input, and they produce a multidimensional output signal. Thanks to this configuration, each output is not just a function of the inputs in its own stage, but depends also on the output of the previous stages. It ensures that each input influences all the following outputs, or, on the other side, an output is a function of all the previous and current stages inputs. A simple illustration of RNNs is  shown in the figure below, wherer the inputs are in the bottom and the outputs in the top:\n",
    "![RNN](./graph/rnn/RNN.jpg)\n",
    "\n",
    "\n",
    "One of the main challenges to RNNS is the vanising/exploding gradient. Its means that with a long RNN, the training phase may lead to very tiny or huge gradients back-propagated throughout the network, which leads the weights to zero or infinity. LSTMs models is an evolution of RNNs to mitigate this problem.\n",
    "\n",
    "Specificaly, the LSTM models have two outputs for each stage: one is the actual output of the model, and the other one, named memory, is the internal state of the stage. Both outputs are fed into the following stages, lowering the chances of having vanishing/exploding gradients. However, this comes with the price of higher model's complexity and larger memory footprint. It is strongly recommended to use GPU devices when training RNNS to speed up the running time. \n",
    "\n",
    "For a more detailed description about RNNs and LSTMs, please refer to this [link](https://towardsdatascience.com/recurrent-neural-networks-and-lstm-4b601dd822a5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stock Price Prediction using LSTM \n",
    "By using LSTM, we can exploit the temporal redundancy contained in our signal. Unlike regression, LSTM need a three dimensional signal as input. Therfore, we need to reformat the observation matrix into a 3D tensor, with three axes:\n",
    "\n",
    "* The first containing the samples\n",
    "* The second containing the timeseries\n",
    "* The third containing the input features\n",
    "\n",
    "Since our input signal is mono-dimensional, the input tensor for the LSTM should have the size (None, *time_dimension*, 1), where *time_dimension* is the length of the time window. \n",
    "\n",
    "Let's start create a script called `rnn_stock_price.py` t\n",
    "First of all, some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from evaluate_ts import evaluate_ts\n",
    "from tensorflow.contrib import rnn\n",
    "from tools import fetch_stock_price, format_dataset\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(101)\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we set the window size to chunk the signal, and the hyperparameters for the model. These parameters are values to hit maximum performance after running a few tests. Feel free to tune them if you want to achieve a better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for the dataset creation\n",
    "symbol = \"MSFT\"\n",
    "time_dimension = 20\n",
    "train_size = 252\n",
    "test_size = 250 - time_dimension\n",
    "\n",
    "# Setting for tensorflow\n",
    "tf_logdir = \"./logs/tf/stock_price_lstm\"\n",
    "os.makedirs(tf_logdir, exist_ok=1)\n",
    "learning_rate = 0.05\n",
    "optimizer = tf.train.AdagradOptimizer\n",
    "n_epochs = 5000\n",
    "n_embeddings = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to fetch the stock price, and reshape it to a 3D tensor shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the values, and prepare the train/test split\n",
    "stock_values = fetch_stock_price(symbol, datetime.date(2015,1,1), datetime.date(2016,12,31))\n",
    "minibatch_cos_X, minibatch_cos_y = format_dataset(stock_values, time_dimension)\n",
    "\n",
    "train_X = minibatch_cos_X[:train_size,:].astype(np.float32)\n",
    "train_y = minibatch_cos_y[:train_size].reshape((-1,1)).astype(np.float32)\n",
    "test_X = minibatch_cos_X[train_size:,:].astype(np.float32)\n",
    "test_y = minibatch_cos_y[train_size:].reshape((-1,1)).astype(np.float32)\n",
    "\n",
    "train_X_ts = train_X[:,:,np.newaxis]\n",
    "test_X_ts = test_X[:,:,np.newaxis]\n",
    "\n",
    "# Create the placeholders\n",
    "X_tf = tf.placeholder(\"float\", shape=(None, time_dimension, 1), name=\"X\")\n",
    "y_tf = tf.placeholder(\"float\", shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model is wrapped inside an `RNN` function as before. This time we want to observer the behavior of the **MAE** and **MSE** using the tool named **Tensorboard**, therefore the body of the `RNN` function should be inside the named-scope LSTM as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM model\n",
    "def RNN(x, weights, biases):\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "        x_ = tf.unstack(x, time_dimension, 1)\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_embeddings)\n",
    "        outputs, _ = rnn.static_rnn(lstm_cell, x_, dtype=tf.float32)\n",
    "        return tf.add(biases, tf.matmul(outputs[-1], weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the **cost** and **optimizer**  function should be wrapped in a Tensorflow scope. Also, we will add the **mae** computation within the tensorflow graph. The weights, biases and predictions are defined as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weights and biases\n",
    "weights = tf.Variable(tf.truncated_normal([n_embeddings, 1], mean=0.0, stddev=10.0), name=\"weights\")\n",
    "biases = tf.Variable(tf.zeros([1]), name=\"bias\")\n",
    "\n",
    "# Model, cost and optimizer\n",
    "y_pred = RNN(X_tf, weights, biases)\n",
    "# Configuration for tensorboard\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.square(y_tf - y_pred))\n",
    "    train_op = optimizer(learning_rate).minimize(cost)\n",
    "    tf.summary.scalar(\"MSE\", cost)\n",
    "\n",
    "with tf.name_scope(\"mae\"):\n",
    "    mae_cost = tf.reduce_mean(tf.abs(y_tf - y_pred))\n",
    "    tf.summary.scalar(\"mae\", mae_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, the main function should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tf_logdir, sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # For each epoch, the whole training set is feeded into the tensorflow graph\n",
    "    for i in range(n_epochs):\n",
    "        summary, train_cost, _ = sess.run([merged, cost, train_op], feed_dict={X_tf: train_X_ts, y_tf: train_y})\n",
    "        writer.add_summary(summary, i)\n",
    "        if i%100 == 0:\n",
    "            print(\"Training iteration\", i, \"MSE\", train_cost)\n",
    "\n",
    "    # After training, check the performance on the test set\n",
    "    test_cost, y_pr = sess.run([cost, y_pred], feed_dict={X_tf:test_X_ts, y_tf:test_y})\n",
    "    print(\"Test dataset:\", test_cost)\n",
    "\n",
    "    # Evaluate the result\n",
    "    evaluate_ts(test_X, test_y, y_pr)\n",
    "\n",
    "    # Visualize the predicted value on both training and test set\n",
    "    plt.plot(range(len(stock_values)), stock_values, 'b')\n",
    "    plt.plot(range(len(stock_values) - test_size, len(stock_values)), y_pr, 'r--')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Predicted and true values\")\n",
    "    plt.title(\"Predicted (Red) VS Real (Blue)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output, using these parameter, is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "Training iteration 0 MSE 29982.285\n",
    "Training iteration 100 MSE 47.04031\n",
    "Training iteration 200 MSE 29.593338\n",
    "Training iteration 300 MSE 18.40517\n",
    "Training iteration 400 MSE 13.436308\n",
    "Training iteration 500 MSE 8.499208\n",
    ". . .\n",
    ". . .\n",
    "Training iteration 4500 MSE 3.5882506\n",
    "Training iteration 4600 MSE 3.5849195\n",
    "Training iteration 4700 MSE 3.5816896\n",
    "Training iteration 4800 MSE 3.578576\n",
    "Training iteration 4900 MSE 3.575548\n",
    "Test dataset: 1.9637392\n",
    "Evaluation of the predictions:\n",
    "MSE (mean squared error):  1.9637395\n",
    "MAE (mean absolute error):  1.0058603\n",
    "Benchmark: if prediction == last feature (prediction without any model)\n",
    "MSE:  124.76326\n",
    "MAE:  9.082401\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error on test set is **1.963739**, which is slightly lower than that of the previous regression model (**2.028827**). However, we would like to see more significant improvement. This goal can be achieved in several ways. The first one is to adjust the models' parameters such as learning rate, number of epochs, number of embeddings. However, the values of these parameters above are already optimal values to hit maximum performance after we tried running multiple tests. Therefore, we will come up with the second approach, which is to add more data to the training set. Instead of having only 2015's historical stock prices as the training set, we will include both 2014 and 2015's historical data in the training set. This can be done easily by changing only two lines of codes in the script above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ". . .\n",
    "train_size = 504\n",
    ". . .\n",
    "stock_values = fetch_stock_price(symbol, datetime.date(2014,1,1), datetime.date(2016,12,31))\n",
    ". . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the other settings remains unchanged. After running the tensorflow session, here is the new output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```JSON\n",
    "Training iteration 0 MSE 26653.746\n",
    "Training iteration 100 MSE 42.37007\n",
    "Training iteration 200 MSE 23.452326\n",
    "Training iteration 300 MSE 16.126783\n",
    "Training iteration 400 MSE 12.027955\n",
    "Training iteration 500 MSE 9.387664\n",
    ". . .\n",
    ". . .\n",
    "Training iteration 4400 MSE 2.666048\n",
    "Training iteration 4500 MSE 2.6683052\n",
    "Training iteration 4600 MSE 2.6700983\n",
    "Training iteration 4700 MSE 2.6643674\n",
    "Training iteration 4800 MSE 2.656452\n",
    "Training iteration 4900 MSE 2.6513186\n",
    "Test dataset: 1.7990497\n",
    "Evaluation of the predictions:\n",
    "MSE (mean squared error):  1.7990497\n",
    "MAE (mean absolute error):  0.9558516\n",
    "Benchmark: if prediction == last feature (prediction without any model)\n",
    "MSE:  124.76326\n",
    "MAE:  9.082401\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new MSE is **1.7990497**, which is **10%** better than that of the regression model (**2.028827**).\n",
    "The predicted vs real values, as well as the prediction error, are shown below:\n",
    "\n",
    "![predicted_vs_real_test_lstm](./graph/rnn/predicted_vs_real_values_test_set.png)\n",
    "\n",
    "![prediction_error](./graph/rnn/predictions_error_trendline.png)\n",
    "\n",
    "![predicted_vs_real_train_test_lstm](./graph/rnn/predicted_vs_real_values_train_test_set.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's launch the `tensorboard`by running the following command in the termial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "tensorboard --logdir=./logs/tf/stock_price_lstm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opening the brower at `localhost:6006`, from the first tab, we can observe the behavior of the MSE and MAE:\n",
    "\n",
    "![tensorboard](./graph/tensorboard_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend looks very nice, it goes down until it reaches a plateau. Also, let's check the `tensorflow` graph (in the tab GRAPH). Here we can see how things are connected together, and how operator are influenced by each other:\n",
    "\n",
    "![lstm_graph](./graph/LSTM_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
